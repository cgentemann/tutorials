{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The Amazon Data Demo\n",
    "This notebook will work through the demo in section 3.2 of the book.  If you do not already have boto3, the amazon python sdk installed, then uncomment and run the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##create an s3 instance object\n",
    "Follow the instructions on the IAM portal to get an access key and a secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "    aws_access_key_id='your access key',\n",
    "    aws_secret_access_key='your secret key'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's test this by creating our bucket \"datacont\" in the Oregon data center.  The creationBucket location is optional, but the location will be encoded into our URLs later.  The creation function will through an exception if the bucket already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this may already exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s3.create_bucket(Bucket='datacont', CreateBucketConfiguration={\n",
    "        'LocationConstraint': 'us-west-2'})\n",
    "except:\n",
    "    print \"this may already exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make this bucket publicly readable.   We will also need to make each blob in the bucket publically readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(\"datacont\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Thu, 07 Jul 2016 18:37:44 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'RM2zILiBLYtOVnnuVsK0j/7YyEsZFdcGF5PnSQO21HxFGz42U88skmpBXuBQsQ3/f/+E7tKPuXI=',\n",
       "   'x-amz-request-id': '2D764B7DE7A58577'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'RM2zILiBLYtOVnnuVsK0j/7YyEsZFdcGF5PnSQO21HxFGz42U88skmpBXuBQsQ3/f/+E7tKPuXI=',\n",
       "  'RequestId': '2D764B7DE7A58577'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket.Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "now let's try to upload a file into the bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#upload a new object into the bucket\n",
    "body = open('path-to-a-file\\exp1', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "o = s3.Object('datacont', 'test').put(Body=body )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Thu, 07 Jul 2016 18:38:33 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'rVO6eBJDldB19+sUQLfv/Zmaq7HBl+UBFhVLpW2AdHFNffUF9LP6koE4XKFZXVf5rt19JIG/zSs=',\n",
       "   'x-amz-request-id': '839011F5955BA066'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'rVO6eBJDldB19+sUQLfv/Zmaq7HBl+UBFhVLpW2AdHFNffUF9LP6koE4XKFZXVf5rt19JIG/zSs=',\n",
       "  'RequestId': '839011F5955BA066'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.Object('datacont', 'test').Acl().put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url for the test item in the buck should be https://s3-us-west-2.amazonaws.com/datacont/test.\n",
    "\n",
    "Next we will create the dynamodb table.  Note that creating the resource does not create the table a table.   the following try-block creates the table.  We need to give a a Key schema.  One element is hashed to produce a partion to store a row, the second key is RowKey.  The pair (PartitionKey, RowKey) is a unique identifier to a row in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dyndb = boto3.resource('dynamodb',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='your access key',\n",
    "    aws_secret_access_key='your secret key'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    table = dyndb.create_table(\n",
    "        TableName='DataTable',\n",
    "        KeySchema=[\n",
    "            {\n",
    "                'AttributeName': 'PartitionKey',\n",
    "                'KeyType': 'HASH'\n",
    "            },\n",
    "            {\n",
    "                'AttributeName': 'RowKey',\n",
    "                'KeyType': 'RANGE'\n",
    "            }\n",
    "        ],\n",
    "        AttributeDefinitions=[\n",
    "            {\n",
    "                'AttributeName': 'PartitionKey',\n",
    "                'AttributeType': 'S'\n",
    "            },\n",
    "            {\n",
    "                'AttributeName': 'RowKey',\n",
    "                'AttributeType': 'S'\n",
    "            },\n",
    "\n",
    "        ],\n",
    "        ProvisionedThroughput={\n",
    "            'ReadCapacityUnits': 5,\n",
    "            'WriteCapacityUnits': 5\n",
    "        }\n",
    "    )\n",
    "except:\n",
    "    #if there is an exception, the table may already exist.   if so...\n",
    "    table = dyndb.Table(\"DataTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wait for the table to be created\n",
    "table.meta.client.get_waiter('table_exists').wait(TableName='DataTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(table.item_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##reading the csv file, uploading the blobs and creating the table\n",
    "We assume that each row of the csv file looks like:\n",
    "    (experimentname, id-number, name-of-ith-file, date, comments)\n",
    "We create a url based on where we know the blobs are stored and append that to the tuple above and insert that list into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiment1', '1', '3/15/2002', 'exp1', 'this is the comment']\n",
      "['experiment1', '2', '3/15/2002', 'exp2', 'this is the comment2']\n",
      "['experiment2', '3', '3/16/2002', 'exp3', 'this is the comment3']\n",
      "['experiment3', '4', '3/16/2002', 'exp4', 'this is the comment233']\n"
     ]
    }
   ],
   "source": [
    "with open('c:\\users\\dennis\\documents\\experiments.csv', 'rb') as csvfile:\n",
    "    csvf = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for item in csvf:\n",
    "        print item\n",
    "        body = open('c:\\users\\dennis\\documents\\datafiles\\\\'+item[3], 'rb')\n",
    "        s3.Object('datacont', item[3]).put(Body=body )\n",
    "        md = s3.Object('datacont', item[3]).Acl().put(ACL='public-read')\n",
    "        \n",
    "        url = \" https://s3-us-west-2.amazonaws.com/datacont/\"+item[3]\n",
    "        metadata_item = {'PartitionKey': item[0], 'RowKey': item[1], \n",
    "                 'description' : item[4], 'date' : item[2], 'url':url} \n",
    "        try:\n",
    "            table.put_item(Item=metadata_item)\n",
    "        except:\n",
    "            print \"item may already be there or another failure\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's search for an item'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'url': u' https://s3-us-west-2.amazonaws.com/datacont/exp4', u'date': u'3/16/2002', u'PartitionKey': u'experiment3', u'description': u'this is the comment233', u'RowKey': u'4'}\n"
     ]
    }
   ],
   "source": [
    "response = table.get_item(\n",
    "    Key={\n",
    "        'PartitionKey': 'experiment3',\n",
    "        'RowKey': '4'\n",
    "    }\n",
    ")\n",
    "item = response['Item']\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Item': {u'PartitionKey': u'experiment3',\n",
       "  u'RowKey': u'4',\n",
       "  u'date': u'3/16/2002',\n",
       "  u'description': u'this is the comment233',\n",
       "  u'url': u' https://s3-us-west-2.amazonaws.com/datacont/exp4'},\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '198',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'date': 'Thu, 07 Jul 2016 18:55:49 GMT',\n",
       "   'x-amz-crc32': '3835589557',\n",
       "   'x-amzn-requestid': 'LBV3KQ5GJTK9I2A85EB4MJ2ENVVV4KQNSO5AEMVJF66Q9ASUAAJG'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'RequestId': 'LBV3KQ5GJTK9I2A85EB4MJ2ENVVV4KQNSO5AEMVJF66Q9ASUAAJG'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
